{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f27ddf-20a9-40a8-9ee3-ad53de1140a7",
   "metadata": {},
   "source": [
    "## 3. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9722f8f5-0e57-4a6f-a613-c5e012a73246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 12:39:17.253287: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-06 12:39:18.425131: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /shared/centos7/cuda/11.7/lib64:/shared/centos7/anaconda3/2022.05/lib:/shared/centos7/nodejs/14.15.4/lib\n",
      "2022-12-06 12:39:18.425230: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /shared/centos7/cuda/11.7/lib64:/shared/centos7/anaconda3/2022.05/lib:/shared/centos7/nodejs/14.15.4/lib\n",
      "2022-12-06 12:39:18.425237: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from AG_News_Classification import AG_News_Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "516e635f-86ef-4bbe-ac2c-639eb39a956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LSTM import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2157d1c3-fc31-4a65-b704-1d76a7fa1120",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "embedding_size = 200\n",
    "learning_rate = 0.001\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "\n",
    "model = LSTM(input_size = vocab_size,\n",
    "             hidden_dim = 128,\n",
    "             lstm_layers = 1,\n",
    "             num_classes = 4)\n",
    "\n",
    "optimizer = torch.optim.NAdam(params = model.parameters(),\n",
    "                              lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a276b470-d3ab-4e0e-a72c-c369cc975de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: GPU\n",
      "\n",
      "Loaded Train, Val and Test data with stratified random sampling\n",
      "\n",
      "\tX_train: (120000,), y_train: (120000,)\n",
      "\tX_val: (3800,), y_val: (3800,)\n",
      "\tX_test: (3800,), y_test: (3800,)\n",
      "\n",
      "Tokenized Train, Val and Test data with vocabulory size = 50000\n",
      "Padded sequences with max-length = 200\n",
      "\n",
      "\tX_train: (120000, 200), y_train: (120000,)\n",
      "\tX_val: (3800, 200), y_val: (3800,)\n",
      "\tX_test: (3800, 200), y_test: (3800,)\n",
      "\n",
      "\n",
      "Train set size: 120000\n",
      "Val set size: 3800\n",
      "Test set size: 3800\n",
      "\n",
      "Loaded data and pre-processed\n",
      "\n",
      "batch size: 16, Num batches(# steps per epoch): 7500\n",
      "\n",
      "Training...\n",
      "\n",
      "Epoch 1/10\n",
      "----------\n",
      "Train Loss: 0.4951 Acc: 0.8171\n",
      "Val Loss: 0.3169 Acc: 0.8929\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train Loss: 0.2581 Acc: 0.9197\n",
      "Val Loss: 0.2706 Acc: 0.9132\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train Loss: 0.1937 Acc: 0.9395\n",
      "Val Loss: 0.2655 Acc: 0.9142\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train Loss: 0.1377 Acc: 0.9552\n",
      "Val Loss: 0.2883 Acc: 0.9139\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train Loss: 0.0966 Acc: 0.9677\n",
      "Val Loss: 0.3416 Acc: 0.9116\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train Loss: 0.0441 Acc: 0.9850\n",
      "Val Loss: 0.4338 Acc: 0.9129\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train Loss: 0.0280 Acc: 0.9902\n",
      "Val Loss: 0.5042 Acc: 0.9105\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-05.\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train Loss: 0.0174 Acc: 0.9943\n",
      "Val Loss: 0.5531 Acc: 0.9116\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train Loss: 0.0160 Acc: 0.9947\n",
      "Val Loss: 0.5800 Acc: 0.9116\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-06.\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train Loss: 0.0150 Acc: 0.9951\n",
      "Val Loss: 0.5826 Acc: 0.9116\n",
      "\n",
      "Training complete in 5m 16s\n",
      "Best val Acc: 0.9142 at epoch:3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = AG_News_Classifier(model = model,\n",
    "                         loss = nn.CrossEntropyLoss(),\n",
    "                         optimizer = optimizer,\n",
    "                         learning_rate = learning_rate,\n",
    "                         num_epochs = num_epochs,\n",
    "                         batch_size = batch_size)\n",
    "\n",
    "clf.loadData(vocab_size=vocab_size, embedding_size=embedding_size)\n",
    "\n",
    "clf.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34e5d027-9cb7-4245-b75f-dd824e2ebebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 91.13157894736842 %\n",
      "Accuracy of World: 89.57894736842105 %\n",
      "Accuracy of Sports: 97.78947368421052 %\n",
      "Accuracy of Business: 88.63157894736842 %\n",
      "Accuracy of Sci & Tech: 88.52631578947368 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
